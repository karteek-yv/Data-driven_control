%\documentclass{beamer}
%\RequirePackage[2020-02-02]{latexrelease}
\documentclass[hyperref={pdfpagemode=FullScreen},aspectratio=169]{beamer}

%    1610: 16:10
%    169: 16:9
%    149: 14:9
%    141: 1.41:1
%    54: 5:4
%    43: 4:3 [default]
%    32: 3:2

\setbeamersize{text margin left=1pt,text margin right=1pt}
\mode<presentation> {
%\setbeamertemplate{caption}[numbered]
\setbeamertemplate{footline}[page number]
\setbeamertemplate{bibliography item}{\insertbiblabel}
\usetheme{default}
\usefonttheme{serif}
}
\usepackage{amsfonts,epsfig,subcaption,graphicx,tabularx,amsmath,bm,relsize,steinmetz,booktabs,epstopdf}
\usepackage{rotating}
\usepackage{extarrows,mathtools}
\usepackage{amsrefs}
\usepackage[english]{babel}
\usepackage{epstopdf}
\usepackage{listings}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{tcolorbox}
\usepackage{arydshln}
\setlength{\dashlinedash}{4pt}
\setlength{\dashlinegap}{4pt}

\usepackage[nameinlink]{cleveref}
\usepackage{aliascnt}
\crefname{equation}{Eqn.}{Eqns.}
\newaliascnt{ineq}{equation}
\aliascntresetthe{ineq}
\crefname{ineq}{Ineq.}{Ineqs.}
\creflabelformat{ineq}{#2{\upshape(#1)}#3} 
\makeatletter
\def\ineq{$$\refstepcounter{ineq}}
\def\endineq{\eqno \hbox{\@eqnnum}$$\@ignoretrue}
\makeatother
\crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
%\renewcommand{\figurename}{Fig.} %without babel package
\addto\captionsenglish{\renewcommand{\figurename}{Fig.}} %with babel package
%
%\usepackage{amsthm}
%\newtheorem{definition}{Definition}

\usepackage[printwatermark]{xwatermark}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{patterns,decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes,arrows,positioning,calc}
\usetikzlibrary{patterns,angles,quotes}
\usepackage{circuitikz}

\newsavebox\mybox
\savebox\mybox{\tikz[color=red,opacity=0.1]\node{\small{Dr. Y. V. Karteek, SCEE, IIT Mandi}};}
\newwatermark*[
  allpages,
  angle=0,
  scale=1,
  xpos=-50,
  ypos=-38
]{\usebox\mybox}

%\title{\Large{UEI303: Techniques on Signals and Systems \\~\\ Random Signals}~\\}
%\title{\Large{UEI407: Signals and Systems\\~\\ Random Signals}~\\}
\title{\Large{EE538: Data-driven Control\\~\\}~\\}
\author{\small Dr. Y. ~V. ~Karteek}
 
\date{} 
%\date{\tiny {\today}}
%----------------------------------------------------
\begin{document}
%----------------------------------------------------
\begin{frame}[plain]
          \begin{center}
                    \Large{}
                    \end{center}
               \titlepage
        \end{frame}
%--------------------------------------------------
\begin{frame}[plain]{Introduction}
\begin{itemize}
\setlength\itemsep{1em}
\item Systems and control theory deals with \textit{controller design problem} for physical systems.
\item Obtaining \textit{mathematical model} of the physical system is the first step.
\item Such a model can be of ODE, PDE, difference equations, transfer functions, transfer matrices etc..
\item The mathematical models are usually based on basic physical laws such as conservation laws, Newton's laws, Kirchhoff's laws, etc..
\item An alternative way to obtain a model is to do experiments on physical system, obtain \textit{data} that can be used to find mathematical descriptions, and is called the \textit{data-driven approach} to control
design.
\item This method is called \textit{system identification}.
\item The desired behaviour require the mathematical model to have certain properties (qualitative or quantitative), forming a design objective.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item Final step is to design a mathematical model of the controller.
\item The above approach is called \textit{model-based control}.
\item Modelling errors due to un-modelled dynamics occur, especially when a complex high order system is represented by a low-order model.
\item In the real world, a full-order model does not exist, and any description is an approximation.
\item The advent of identification theory solved the problem of controlling complex time-varying plants using model-based control design.
\item Modelling by physical laws or by identification from data, modelling errors are inescapable, and explicit quantification of modelling errors is practically impossible.
\item This led to the development of the model-based approaches of fixed-parameter robust control and adaptive control system design.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item The mathematical models derived from the physical laws have been effectively used in practical applications, provided that the following assumptions hold:
\begin{itemize}\setlength\itemsep{1em}
\item Accurately model the actual plant.
\item Priori bounds on the noise and modelling errors are available.
\end{itemize}
\item The identified model can capture the main features of the plant, provided that
\begin{itemize}
\setlength\itemsep{1em}
\item Compatibility of the selected model structure and parametrisation with the actual plant's characteristics is assumed.
\item The experiment design is appropriate; for control problems, the selection of the input signal is in accordance with the actual plant's characteristics or the persistence of excitation (PE) condition.
\end{itemize}
\item  In summary, if an accurate model is unavailable, or the assumptions regarding the plant do not hold, the designed model-based controller, validated by simulations, can lead to an unstable closed-loop plant or poor closed-loop performance.
\item Adaptive and robust control systems have successfully controlled many real-world and industrial plants. 
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item However, both strategies require many prior plant assumptions to be mandated by the theory.
\item The alternative approach deals with the problem of synthesizing control laws directly on the basis of measured data.
\item The combination of system identification and model-based control as described above is an instance of data-driven control design.
\item Methods using this combination are often called \textit{indirect} methods of data-driven control, consisting of the two-step process of data-driven modelling (i.e., system identification) followed by model-based control.
\item \textit{Direct} approaches focus on directly mapping data to controllers without an intermediate step of system identification.
\item Both paradigms have different pros and cons.
\item Identification might be expensive and the obtained model may not always be useful for the intended control design problem.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item Unique system identification may be impossible, because the data are corrupted by noise and do not contain sufficient information
\item Direct data-driven control design has the premise of being an end-to-end approach, requiring less expert knowledge.
\item However, in comparison to the maturity of system identification, the theory of direct data-driven control is still very much under development.
\item  In the 1990s and early 2000s, a number of direct data-driven control schemes emerged, including iterative feedback tuning (IFT) and virtual reference feedback tuning (VRFT).
\item These methods both aim at using data to directly minimize a cost function of the control parameters, with the notable distinction that IFT is an iterative approach while VRFT is one-shot.
\item Result on \textit{fundamental lemma} asserts that all finite-length trajectories of a controllable linear system can be obtained from a single one whose input is \textit{persistently exciting}.
\item It provides conditions on the input data that enable system identification.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item The recent interest in direct data-driven simulation and tracking was motivated by the development of low cost sensing devices, and data were by now widely available.
\item Increase in available computational power to analyse large datasets also fuelled that interest.
\item Consider a linear time-invariant (LTI) system:
\begin{subequations}\label{eq1}
\begin{align}
x(t+1) &= A_\text{true}x(t) + B_\text{true}u(t)\\
y(t) &= C_\text{true}x(t) + D_\text{true}u(t)
\end{align}
\end{subequations}
where $x(t)\in \mathbb{R}^{n_\text{true}}$ is the state, $u(t)\in \mathbb{R}^m$ is the input, $y(t)\in \mathbb{R}^p$ is the output.
\item The true state-space dimension $n_\text{true}$ and the matrices $A_\text{true}$, $B_\text{true}$, $C_\text{true}$, and $D_\text{true}$ are assumed to be unknown.
\item An upper bound $N$ on the state-space dimension is given, $N\geq n_\text{ture}$.
\item The goal is to simulate and/or control the dynamics of \cref{eq1} using input-output data.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{0.5em}
\item Consider a restricted input-output trajectory of interval $[0,T-1]$
\item For a given integer $L\in [1,T]$, we define the Hankel matrix of depth $L$ as:
\begin{align}\label{eq2}
\begin{bmatrix}
H_L(u_{[0,T-1]}) \\ 
\hdashline 
H_L(y_{[0,T-1]})
\end{bmatrix} = \begin{bmatrix}
u(0) & u(1) & \cdots & u(T-L) \\
\vdots & \vdots & & \vdots \\
u(L-1) & u(L) & \cdots & u(T-1) \\
\hdashline 
y(0) & y(1) & \cdots & y(T-L) \\
\vdots & \vdots & & \vdots \\
y(L-1) & y(L) & \cdots & y(T-1)
\end{bmatrix}
\end{align}
\item By time-invariance of the system, each column of \cref{eq2} gives rise to a restricted input-output trajectory of \cref{eq1} on the time interval $[0, L-1]$.
\item By linearity of the system, every linear combination of the columns of \cref{eq2} is also a restricted input-output trajectory on the time interval $[0, L-1]$.
\item Every restricted input-output trajectory of length $L$ can be expressed as a linear combination of the columns of \cref{eq2}, assuming that the system in \cref{eq1} is controllable, and $u_{[0,T-1]}$ is persistently exciting of sufficiently high order.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain,allowframebreaks]{}
\begin{definition}
Let $k\in [1,T]$ be an integer. The input $u_{[0,T-1]}$ is called persistently exciting of order $k$ if $H_k(u_{[0,T-1]})$ has full row rank.
\end{definition}
\begin{theorem}
Assume that the pair  $\left(A_\text{true}, B_\text{true}\right)$ is controllable. Consider a restricted input-state-output trajectory $\left(u_{[0,T-1]} , x_{[0,T-1]} , y_{[0,T-1]}\right)$ of \cref{eq1}, where $T\geq1$. Let $L\in [1,T]$ be an integer. If the input $u_{[0,T-1]}$ is persistently exciting of order $N+L$, then the following statements hold:
\end{theorem}
\begin{enumerate}
\item The matrix
\begin{align}\label{eq3}
\begin{bmatrix}
X_{[0,T-L]} \\ 
\hdashline 
H_L(u_{[0,T-1]})
\end{bmatrix} = \begin{bmatrix}
x(0) & x(1) & \cdots & x(T-L) \\
\hdashline 
u(0) & u(1) & \cdots & u(T-L) \\
\vdots & \vdots & & \vdots \\
u(L-1) & u(L) & \cdots & y(T-1)
\end{bmatrix}
\end{align}
has full rank.
\item $\left(\bar{u}_{[0,L-1]}, \bar{y}_{[0,L-1]} \right)$ is a restricted input-output trajectory of \cref{eq1} on the time interval $[0,L-1]$ if and only if
\begin{align}\label{eq4}
\begin{bmatrix}
\bar{u}_{[0,L-1]}\\
\bar{y}_{[0,L-1]}
\end{bmatrix} = \begin{bmatrix}
H_L(u_{[0,T-1]}) \\
H_L(y_{[0,T-1]})
\end{bmatrix}g
\end{align}
for some vector $g\in \mathbb{R}^{T-L+1}$
\item For $i\in \mathbb{Z}_+, \left(\bar{u}_{[i,i+L-1]}, \bar{y}_{[i,i+L-1]} \right)$ is a restricted input-output trajectory of \cref{eq1} on the time interval $[i, i+L-1]$ if and only if
\begin{align}\label{eq5}
\begin{bmatrix}
\bar{u}_{[i,i+L-1]}\\
\bar{y}_{[i,i+L-1]}
\end{bmatrix} = \begin{bmatrix}
H_L(u_{[0,T-1]}) \\
H_L(y_{[0,T-1]})
\end{bmatrix}g
\end{align}
for some $g\in \mathbb{R}^{T-L+1}$
\end{enumerate}
We note that the condition of persistency of excitation requires a sufficiently long trajectory,
\begin{align}
T\geq (m+1)(N+L)-1
\end{align}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item The confluence of big data and machine learning is driving a paradigm shift in the analysis and understanding of dynamical systems in science and engineering.
\item In some applications such as climate science, finance, epidemiology, and neuroscience, data are abundant, however, the physical laws or governing equations are hard to find. 
\item Consider a dynamical system of the form 
\begin{align}
\dot{x}(t)=f(x(t),t;\beta)
\end{align}
which can represent a wide class of systems (linear and nonlinear ).
\item The general linear dynamics are of the form
\begin{align}\label{eq8}
\dot{x}=Ax
\end{align}
\item The solution of \cref{eq8} is given by,
\begin{align}
x(t+t_0) = e^{At}x(t_0)
\end{align}
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item The dynamics are entirely characterized by the eigenvalues and eigenvectors of the matrix $A$.
\item The spectral decomposition (eigendecomposition) of $A$ is given by:
\begin{align}
AT = T\Lambda
\end{align}
\item When $A$ has $n$ distinct eigenvalues, then $\Lambda$ is a diagonal matrix containing the eigenvalues $\lambda_j$ and $T$ is a matrix whose columns are the linearly independent eigenvectors $\xi_j$ associated with eigenvalues $\lambda_j$.
\item The solution of \cref{eq8} can also be written as
\begin{align}
x(t+t_0) = Te^\Lambda T^{-1}x(t_0)
\end{align}
\item In the case of repeated eigenvalues, the matrix $\Lambda$ will consist of Jordan blocks.
\item Nonlinearity remains a primary challenge in analyzing and controlling dynamical systems, giving rise to complex global dynamics.
\item Linear systems may be completely characterized in terms of the spectral decomposition (i.e., eigenvalues and eigenvectors) of the matrix $A$.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item No such overarching framework exists for nonlinear systems.
\item Nonlinear dynamical systems consider the geometry of subspaces of local linearizations around fixed points.
\item It is often possible to apply  linear analysis techniques in a small neighborhood of a fixed point or periodic orbit.
\item Even more central challenge arises from the lack of known governing equations for many modern systems of interest .
\item In complex and realistic systems, such as are found in neuroscience, epidemiology, and ecology,  there is a basic lack of known physical laws.
\item Traditionally, physical systems were analyzed by making ideal approximations and then deriving simple differential equation models.
\item All models are approximations, and, with increasing complexity, these approximations often become suspect. 
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item Determining what is the correct model is becoming more subjective.
\item There are also often latent variables that are relevant to the dynamics but may go unmeasured. \item Uncovering these hidden effects is a major challenge for data-driven methods.
\item Identifying unknown dynamics from data and learning intrinsic coordinates that enable the linear representation of nonlinear systems are two of the most pressing goals of modern dynamical systems.
\item \textbf{Dynamic mode decomposition} (DMD) is developed to identify spatio-temporal coherent structures from high-dimensional data in the fluid dynamics.
\item DMD is based on proper orthogonal decomposition (POD), which utilizes the computationally efficient singular value decomposition (SVD).
\item It scales well to provide effective dimensionality reduction in high-dimensional systems.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item DMD provides a modal decomposition where each mode consists of spatially correlated structures that have the same linear behaviour in time.
\item DMD provides not only dimensionality reduction in terms of a reduced set of modes, but also a model for how these modes evolve in time.
\item DMD may be formulated as an algorithm to identify the best-fit linear dynamical system that advances high-dimensional measurements forward in time
\item DMD is equally valid for experimental and numerical data, as it is not based on knowledge of the governing equations, but is instead based purely on measurement data.
\item Each DMD mode is associated with a particular eigenvalue $\lambda = a+ib$, with a particular frequency of oscillation $b$ and growth or decay rate $a$.
\item Exact DMD is based on the efficient and numerically well-conditioned singular value decomposition.
\item DMD is inherently data-driven, and the first step is to collect a number of pairs of snapshots of the state of a system as it evolves in time.
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item These snapshot pairs may be denoted by $\{(x(t_k),x(t'_k))\}_{k=1}^{m}$, where $t'_k=t+\Delta t$, and the time-step $\Delta t$ is sufficiently small to resolve the highest frequencies in the dynamics. 
\item These snapshots are then arranged into two data matrices, $X$ and $X'$:
\begin{align}
X&=\begin{bmatrix}
| & | & &|\\
x(t_1) & x(t_2) & \cdots & x(t_m)\\
| & | & &|\\
\end{bmatrix} \\
X'&=\begin{bmatrix}
| & | & &|\\
x'(t_1) & x'(t_2) & \cdots & x'(t_m)\\
| & | & &|\\
\end{bmatrix}
\end{align}
\item The original formulations assumed uniform sampling in time, so that $t_k=k\Delta t$ and $t'_k=t_k+\Delta t = t_{k+1}$.
\item If we assume uniform sampling in time, we will adopt the notation $x_k = x(k\Delta t)$.
\item The DMD algorithm seeks the leading spectral decomposition (i.e., eigenvalues and eigenvectors) of the best-fit linear operator $A$ that relates the two snapshot matrices in time:
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{align}
X'\approx AX
\end{align}
\begin{itemize}
\setlength\itemsep{1em}
\item The best-fit operator A then establishes a linear dynamical system that best advances snapshot measurements forward in time. 
\item If we assume uniform sampling in time, 
\begin{align}
x_{k+1} \approx Ax_k
\end{align}
\item Mathematically, the best-fit operator $A$ is defined as
\begin{align}\label{eq16}
A = \underset{A}{\text{argmin}} \lVert X'-AX\rVert_F = X'X^\dagger
\end{align}
\item Matrix $A$closely resembles the Koopman operator (shall be discussed later).
\item Because $A$ is an approximate representation of the Koopman operator restricted to a finite-dimensional subspace of linear measurements, we are often interested in the eigenvectors $\Phi$ and eigenvalues $\Lambda$ of $A$.
\begin{align}
A = \Phi A \Phi^{-1}
\end{align}

\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item For a high dimensional state vector $x\in \mathbb{R}^n$, the matrix $A$ has $n^2$ elements, computing its spectral 	decomposition may be intractable.
\item The DMD algorithm leverages dimensionality reduction to compute the dominant eigenvalues and eigenvectors of $A$ without requiring any explicit computations using $A$ directly.
\item The pseudo-inverse $X^\dagger$ in \cref{eq16} is computed via the singular value decomposition of the matrix $X$.
\item This matrix typically has far fewer columns than rows, i.e., $m \ll n$.
\item There are at most $m$ non-zero singular values and corresponding singular vectors, and hence the matrix $A$ will have at most rank $m$. 
\item Instead of computing $A$ directly, we compute the projection of $A$ onto these leading singular vectors, resulting in a small matrix $\tilde{A}$ of size at most $m \times m$.
\item These approximate modes are in fact exact eigenvectors of the full $A$ matrix under certain conditions
\end{itemize}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain,allowframebreaks]{DMD Algorithm}
\begin{enumerate}
\setlength\itemsep{1em}
\item Compute the singular value decomposition of $X$
\begin{align}
X\approx \tilde{U}\tilde{\Sigma}\tilde{V}^*
\end{align}
where $\tilde{U} \in \mathbb{C}^{n\times r}$, $\tilde{\Sigma} \in \mathbb{C}^{r\times r}$, and $\tilde{V} \in \mathbb{C}^{m\times r}$, and $r\leq m$ denotes either the exact or the approximate rank of the data matrix $X$.
\item[] Choosing the approximate rank $r$ is one of the most important and subjective steps in DMD, and in dimensionality reduction in general.
\item[] The columns of the matrix $\tilde{U}$ are also known as POD modes, and they satisfy $\tilde{U}*\tilde{U}=I$.
\item[] Similarly, the columns of $\tilde{V}$ are orthonormal and satisfy $\tilde{V}*\tilde{V}=I$.
\item According to \cref{eq16}, the full matrix $A$ may be obtained by computing the pseudo-inverse of $X$.
\begin{align}
A=X'\tilde{V}\tilde{\Sigma}^{-1}\tilde{U}^*
\end{align}
\item[] We are only interested in the leading $r$ eigenvalues and eigenvectors of $A$, and we may thus project $A$ onto the POD modes in $U$:
\begin{align}
\tilde{A}=\tilde{U}^*A\tilde{U} = \tilde{U}^*X'\tilde{V}\tilde{\Sigma}^{-1}
\end{align}
\item[] Reduced matrix $\tilde{A}$ has the same non-zero eigenvalues as the full matrix $A$.
\item[] We need only compute the reduced $\tilde{A}$, without working with the high-dimensional $A$.
\item[] The $\tilde{A}$ defines a linear model for the dynamics of the vector of POD coefficients $\tilde{x}$.
\begin{align}
\tilde{x}_{k+1} = \tilde{A}\tilde{x}_k
\end{align}
\item[] Matrix $\tilde{U}$ provides a map to reconstruct the full state $x$ from the reduced state $\tilde{x}$.
\item The spectral decomposition of $\tilde{A}$ is given by
\begin{align}
\tilde{A} = W\Lambda W^{-1}
\end{align}
\item[] The entries of the diagonal matrix $\Lambda$ are the DMD eigenvalues, which also correspond to eigenvalues of the full $A$ matrix.
\item[] The columns of $W$ are eigenvectors of $\tilde{A}$, and provide a coordinate transformation that diagonalizes the matrix.
\item[] These columns may be thought of as linear combinations of POD mode amplitudes.
\item[] They behave linearly with a single temporal pattern given by $\lambda$.
\end{enumerate}
\end{frame}
%--------------------------------------------------
\begin{frame}[plain]{}
\begin{itemize}
\setlength\itemsep{1em}
\item 
\end{itemize}
\end{frame}
%--------------------------------------------------

\end{document}
